# -*- coding: utf-8 -*-
"""TeamH_Project_MainDriver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DX-y7IzyW7Q0CiR8ipr3WSCsa3sCVwcF

## Data606 - Capstone Project
```
Group H
Malav Patel, Kent Butler
Prof. Unal Sokaglu
```

This project is about performing time-series analysis on climate data analysis data.

# Research

### References

Some explanations of earth sciences statistics:
https://pjbartlein.github.io/REarthSysSci/ltms-and-anomalies.html

NOAA PSL NCEP-NCAR datasets:  https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html

NOAA PSL, other recognized data sources directory: https://psl.noaa.gov/data/help/othersources/

Global environmental policy timeline, https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline

OECD convergence of policy, climate,and economy: https://www.oecd.org/

NASA climate time machine: https://climate.nasa.gov/interactives/climate-time-machine

### Factoids

* All of the plastic waste produced in the world in 2019 alone weighs as much as 35,000 Eiffel Towers â€“ 353 million tons  - [*Organization for Economic Cooperation and Development (OECD)*](https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline)

## Application Parameters

Note: algorithm tuning is done with declaration of the model.
"""

import pandas as pd
from datetime import datetime as dt
import datetime

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
plt.rcParams["figure.figsize"] = (10,6)
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn')

# Import local source
from projectutil import *
from modelexecutor import ModelExecutor


DRIVE_PATH = "/data/projects/climate-data-model"
# Set the location of this script in GDrive
SCRIPT_PATH = DRIVE_PATH + "/py-src/"
# Root Path of the data on the cloud drive
DATA_ROOT = DRIVE_PATH + "/data/"
# Location of logged output prediction data
LOG_PATH = DATA_ROOT + "/preds/"
# Journal file
JOURNAL_LOG = SCRIPT_PATH + "cv-results.csv"

# Start including data from this date
START_DATE =  pd.to_datetime(dt.fromisoformat('1950-01-01'))
# Stop including data after this date
END_DATE = pd.to_datetime(dt.fromisoformat('2015-12-01'))

"""**Dataset Definitions**"""

# Label to predict
TARGET_LABEL = 'landSeaAvgTemp'

# Base dataset
TEMP_DATA = {'filename':'GlobalTemperatures.csv',
             'feature_map':{'LandAndOceanAverageTemperature':'landSeaAvgTemp'},
             'date_col':'dt'}

#TEMP_DATA = {'filename':'GlobalTemperatures.csv',
#              'feature_map':{'LandAverageTemperature':'landAvgTemp',	'LandMaxTemperature':'landMaxTemp',	'LandMinTemperature':'landMinTemp',	'LandAndOceanAverageTemperature':'landSeaAvgTemp'},
#               'date_col':'dt'}

# Datasets
CO2_DATA = {'filename':"atmospheric-co2.csv",
            'feature_map':{'Carbon Dioxide (ppm)':'co2', 'Seasonally Adjusted CO2 (ppm)':'co2_seas'},
            'date_map':{'Year':'year','Month':'month'}}

SEAICE_DATA = {'filename':"seaice.csv",
               'feature_map':{'     Extent':'ice_extent'},
               'date_map':{' Month':'month','Year':'year',' Day':'day'}}

WEATHER_DATA = {'filename':"finalDatasetWithRain.csv",
                'feature_map':{'air_x':'air_x','air_y':'air_y','uwnd':'uwnd'},
                'date_col':'time'}

VOLCANO_DATA = {'filename':'eruptions-conditioned.csv',
                'feature_map':{'vei':'volcanic_idx'},
                'date_map':{'start_year':'year','start_month':'month'}}

FOREST_DATA = {'filename':'WorldForestCover-Interpolated.csv',
               'feature_map':{'PctCover-Int':'pct_forest_cover'},
               'date_col':'date'}

SUNSPOT_DATA = {'filename':'sunspotnumber.csv',
               'feature_map':{'suns_spot_number':'sunspot_num'},
               'date_map':{'year':'year'}}

POLICY_DATA = {'filename':'GlobalEnvPolicies.csv',
               'feature_map':{'EventRating':'policy_rating'},
               'date_col':'date'}

#GHG_DATA = {'filename':'greenhouse_gas_inventory_data.csv',
#            'feature_map':{''},
#            'date_map':{'Year':'year'}}

"""**Run Parameters**"""

debug = False
show_graphics = False
predict = False
NUM_LOOPS = 10

"""**Hyperparams**"""

SHIFT = 1
# Ratio of test data to train data - used for split
TEST_RATIO = 0.2
# 0..1 percent of data to use as validation
VALIDATION_RATIO = 0.1
# Num epochs
NUM_EPOCHS = 300

# Turn off graphical blocking
if (not show_graphics):
  plt.ion()

# History lookback in network
#INPUT_WINDOWS = [30,45,60]
#INPUT_WINDOWS = [24,36,48]
INPUT_WINDOWS = [84]
LABEL_WINDOWS = [60]

# Models to CV
# 'Densev1',
#MODEL_NAMES = ['Densev1','Densev11','LSTMv3','LSTMv32']
MODEL_NAMES = ['Densev1','Densev11','LSTMv3']

ALL_DATASETS = [[CO2_DATA],
  [CO2_DATA,FOREST_DATA],
  [CO2_DATA,SEAICE_DATA],
  [CO2_DATA,POLICY_DATA],
  [CO2_DATA,SEAICE_DATA,POLICY_DATA],
  [CO2_DATA,SEAICE_DATA,WEATHER_DATA],
  [CO2_DATA,SEAICE_DATA,WEATHER_DATA,FOREST_DATA],
  [CO2_DATA,SEAICE_DATA,WEATHER_DATA,FOREST_DATA,VOLCANO_DATA],
  [CO2_DATA,SEAICE_DATA,WEATHER_DATA,FOREST_DATA,VOLCANO_DATA,SUNSPOT_DATA],
  [CO2_DATA,VOLCANO_DATA],
  [CO2_DATA,VOLCANO_DATA,FOREST_DATA],
  [CO2_DATA,VOLCANO_DATA,FOREST_DATA,SEAICE_DATA],
  [CO2_DATA,VOLCANO_DATA,FOREST_DATA,SEAICE_DATA,SUNSPOT_DATA],
  [CO2_DATA,FOREST_DATA],
  [CO2_DATA,FOREST_DATA,SEAICE_DATA],
  [CO2_DATA,FOREST_DATA,SEAICE_DATA,SUNSPOT_DATA],
  [VOLCANO_DATA],
  [VOLCANO_DATA,FOREST_DATA],
  [VOLCANO_DATA,POLICY_DATA],
  [VOLCANO_DATA,FOREST_DATA,SUNSPOT_DATA],
  [VOLCANO_DATA,FOREST_DATA,SUNSPOT_DATA,POLICY_DATA],
  [VOLCANO_DATA,FOREST_DATA,SUNSPOT_DATA,POLICY_DATA,SEAICE_DATA],
  [FOREST_DATA],
  [FOREST_DATA,POLICY_DATA],
  [SEAICE_DATA],
  [SEAICE_DATA,FOREST_DATA],
  [SEAICE_DATA,POLICY_DATA],
  [SEAICE_DATA,FOREST_DATA,VOLCANO_DATA],
  [SEAICE_DATA,FOREST_DATA,VOLCANO_DATA,POLICY_DATA]
]
ALL_DATASETS = [
  # [VOLCANO_DATA,POLICY_DATA],
#  [VOLCANO_DATA,POLICY_DATA],
#  [FOREST_DATA,POLICY_DATA],
#  [SEAICE_DATA,VOLCANO_DATA,FOREST_DATA,SUNSPOT_DATA,POLICY_DATA],
#  [SEAICE_DATA,VOLCANO_DATA,FOREST_DATA,SUNSPOT_DATA,CO2_DATA,POLICY_DATA],
#  [CO2_DATA,SEAICE_DATA,WEATHER_DATA,VOLCANO_DATA,FOREST_DATA,POLICY_DATA],
#   [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA],
  [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA, POLICY_DATA],
  # [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA],
  # [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA, POLICY_DATA],
  # [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA],
  # [SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA, POLICY_DATA],
]
#ALL_DATASETS=[ALL_DATASETS[0]]
#ALL_DATASETS=[[SEAICE_DATA, VOLCANO_DATA, FOREST_DATA, SUNSPOT_DATA, CO2_DATA, WEATHER_DATA, POLICY_DATA]]
#ALL_DATASETS=[[SEAICE_DATA]]

"""# Execute Trainer"""
for n in range(NUM_LOOPS):
  for i,win in enumerate(INPUT_WINDOWS):
    for j, lab in enumerate(LABEL_WINDOWS):
      for k,model in enumerate(MODEL_NAMES):
        for m,ds_list in enumerate(ALL_DATASETS):
          fnames = [ds['filename'] for ds in ds_list]
          print(f'============================ Executing {i+j+k+m+n} ===================================\n{model}-{win}/{lab}-{fnames}')

          # re-construct the model exec b/c it contains some state
          exec = ModelExecutor(data_path=DATA_ROOT, log_path=LOG_PATH, journal_log=JOURNAL_LOG, start_date=START_DATE, end_date=END_DATE,
                              input_window=win, label_window=lab, shift=SHIFT, test_ratio=TEST_RATIO, val_ratio=VALIDATION_RATIO,
                              num_epochs=NUM_EPOCHS, target_label=TARGET_LABEL, model_name=model, debug=True)

          exec.load_initial_dataset(TEMP_DATA['filename'], TEMP_DATA['feature_map'], date_map=None, date_col=TEMP_DATA['date_col'])

          exec.load_datasets(ds_list)
          #exec.print_correlations()
          exec.process()

  print(f'=========================== Completed {i+j+k+m+n} ===================================')
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Merge Generic Dataset\n",
        "\n",
        "---\n",
        "\n",
        "Given a time-correlated dataset, do the following:\n",
        "* visualize\n",
        "* expose and deal with missing data\n",
        "* create a date column as a merge point\n",
        "* merge w/ given aggregate dataset\n",
        "\n",
        "\n",
        "Requires:\n",
        "* Project_Util"
      ],
      "metadata": {
        "id": "MDnCzDVoM0ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "WWu_MYu0lkNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Merger:\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  def __init__(self, data_path, start_date, end_date, freq='1M', debug=False):\n",
        "    self.data_path = data_path\n",
        "    self.START_DATE = start_date\n",
        "    self.END_DATE = end_date\n",
        "    self.FREQ = freq\n",
        "    self.DEBUG = debug\n",
        "\n",
        "    if (len(data_path) <= 0):\n",
        "      raise AssertionError('Non-zero data_path required')\n",
        "    if (not start_date or not end_date):\n",
        "      raise AssertionError('Date bounds required')\n",
        "\n",
        "    # This is the common merge date field name\n",
        "    self.DATE_COL='_date_'\n",
        "\n",
        "    # Ordinal set for scale levels, used for comparison\n",
        "    self.FREQ_SCALE = {'Y':0, 'M':1, 'D':2}\n",
        "\n",
        "  def merge_dataset(self, filename, feature_map, df_aggr=None, impute_method='ffill', date_map=None, date_col=None):\n",
        "    \"\"\"\n",
        "    Load data from the given data_path.\n",
        "    Crop data to given date bounds.\n",
        "    If date_col is set, will use this to create dates.\n",
        "    Otherwise supply a  date_map, mapping columns to 'year','month', and 'day'.\n",
        "    Re-code 'date' as a pd.timestamp.\n",
        "    Merge into given df_aggr.\n",
        "    \"\"\"\n",
        "    df = self.load_data(filename)\n",
        "    return self.merge_df(df, feature_map, df_aggr, impute_method, date_map, date_col, filename)\n",
        "\n",
        "  def merge_df(self, df, feature_map, df_aggr=None, impute_method='ffill', date_map=None, date_col=None, dataset_name='Unknown'):\n",
        "    \"\"\"\n",
        "    Merge data in the given dataframe into given aggregated df.\n",
        "    Crop data to given date bounds.\n",
        "    If date_col is set, will use this to create dates.\n",
        "    Otherwise supply a  date_map, mapping columns to 'year','month', and 'day'.\n",
        "    Re-code 'date' as a pd.timestamp.\n",
        "    Merge into given df_aggr.\n",
        "    \"\"\"\n",
        "    if (df_aggr is None):\n",
        "      self.debug('Defaulting df_aggr to reference df')\n",
        "      df_aggr = self.get_reference_df()\n",
        "      # ensure it has a date column for later joins\n",
        "      df_aggr[self.DATE_COL] = df_aggr.index\n",
        "\n",
        "    # Grab real column names\n",
        "    COLS = list(feature_map.values())\n",
        "\n",
        "    # GUARD\n",
        "    if (len(COLS) <= 0):\n",
        "      raise AssertionError(f'{filename} - Provide at least one feature_map entry')\n",
        "\n",
        "    # Rename columns\n",
        "    self.debug('Renaming columns', banner=True)\n",
        "    df.rename(columns=feature_map, inplace=True)\n",
        "    if (date_map is not None):\n",
        "      df.rename(columns=date_map, inplace=True)\n",
        "\n",
        "    df.info()\n",
        "\n",
        "    # Groom Dates\n",
        "    # - create std date column\n",
        "    df = self.preprocess_dates(df, date_col)\n",
        "\n",
        "    # Drop unnecessary columns    if (target_freq <= src_freq):\n",
        "\n",
        "    # - this will retain std date column; ensure that exists first\n",
        "    df = self.trim_columns(df, COLS)\n",
        "\n",
        "    # Truncate by date\n",
        "    df = self.snip_dates(df)\n",
        "\n",
        "    print(df.describe())\n",
        "\n",
        "    self.debug('Assessing data freq scales')\n",
        "    # assess granularity of the incoming dataset\n",
        "    #    and translate into a usable code\n",
        "    src_freq = self.assess_granularity(df, COLS)\n",
        "    src_freq = self.FREQ_SCALE[src_freq]\n",
        "    target_freq = self.FREQ_SCALE[self.FREQ[-1]]\n",
        "    self.debug(f'src/target: {src_freq}/{target_freq}')\n",
        "\n",
        "##    if (target_freq <= src_freq):\n",
        "      # Downscale, even if already on same scale, will align dates\n",
        "#      df = self.downscale(df, columns=COLS)\n",
        "#    else:\n",
        "#      df = self.upscale(df, columns=COLS)\n",
        "    df = self.downscale(df, src_freq, columns=COLS)\n",
        "\n",
        "    # Impute Missing Data\n",
        "    df = self.handle_missing(df, COLS, impute_method)\n",
        "\n",
        "    # Manual validation of time intervals\n",
        "    self.print_time_intervals(df)\n",
        "\n",
        "    # Check num rows as a cheap way to see if dates align\n",
        "    #   if more rows - do the averaging down\n",
        "    #   if less rows - impute up\n",
        "    self.debug(f'df/df_aggr size: {df.shape[0]}/{df_aggr.shape[0]}')\n",
        "    #self.debug(f'df and df_aggr shapes equivalent...equal? {df.index == df_aggr.index}')\n",
        "    self.debug(f'pre-merge df::\\n{df}')\n",
        "    self.debug(f'pre-merge df_aggr::\\n{df_aggr}')\n",
        "\n",
        "    # Merge dataset\n",
        "    self.debug('Merging by date')\n",
        "    df_merge = df_aggr.merge(df, on=self.DATE_COL, how='inner', suffixes=['', '_df'])\n",
        "    #TODO: necessary/ harmful??\n",
        "    df_merge = clean_df(df_merge, ['df'])\n",
        "\n",
        "    # Plot Data\n",
        "    self.plot(df_merge, COLS, dataset_name)\n",
        "\n",
        "    return df_merge\n",
        "\n",
        "  def debug (self, _str, banner=False):\n",
        "    if self.DEBUG:\n",
        "      self.log(_str, banner, prefix='###')\n",
        "\n",
        "  def log (self, _str, banner=False, prefix=''):\n",
        "    print('------------------------------------------\\n' if banner else '',\n",
        "          f'{prefix} {_str}')\n",
        "\n",
        "  def get_reference_df(self):\n",
        "    \"\"\"\n",
        "    Create an empty DataFrame with index set up with dates in a regular pattern\n",
        "    for the currently set start/end dates and given frequency.\n",
        "    See https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n",
        "    for the full frequency specification.\n",
        "    \"\"\"\n",
        "    # Generate index\n",
        "    return create_timeindexed_df(self.START_DATE, self.END_DATE, self.FREQ)\n",
        "\n",
        "  def load_data(self, filename):\n",
        "    loadfile = self.data_path + filename\n",
        "    self.debug(f' ============================================================\\n  Loading data{loadfile}\\n',\n",
        "      '============================================================')\n",
        "    return pd.read_csv(loadfile)\n",
        "\n",
        "  def handle_missing(self, df, COLS, impute_method):\n",
        "    # Handle missing values\n",
        "    self.debug(f'Handle missing values via method: {impute_method}...', banner=True)\n",
        "    self.debug('isna() value counts::')\n",
        "    df[COLS].isna().value_counts()\n",
        "\n",
        "    for col in COLS:\n",
        "      df[col].fillna(method=impute_method, inplace=True)\n",
        "\n",
        "    self.debug('isna() value counts::')\n",
        "    df[COLS].isna().value_counts()\n",
        "    return df\n",
        "\n",
        "  def preprocess_dates(self, df, date_col=None):\n",
        "    \"\"\"\n",
        "    Does what create_std_date() does but also installs a Datetime index.\n",
        "    Returns reworked df.\n",
        "    \"\"\"\n",
        "    self.debug(f'preprocess_dates with date_col: {date_col}', banner=True)\n",
        "    if ('DatetimeIndex' in str(type(df.index))):\n",
        "      # Start here\n",
        "      if (self.DATE_COL in df.columns):\n",
        "        # This appears to create a second _date_ col - skip\n",
        "        self.debug('Retaining existing DatetimeIndex; done')\n",
        "      else:\n",
        "        self.debug('Converting DatetimeIndex into a column')\n",
        "        df.reset_index(inplace=True)\n",
        "        df.rename(columns={'index':self.DATE_COL}, inplace=True)\n",
        "\n",
        "    elif (date_col is not None):\n",
        "        self.debug(f'Convert from given col {date_col}')\n",
        "        df[self.DATE_COL] = pd.to_datetime(df[date_col])\n",
        "\n",
        "    # Ensure DATE_COL and year/month/date cols\n",
        "    df = self.create_std_date(df)\n",
        "\n",
        "    # Set date column onto the index\n",
        "    df.set_index(self.DATE_COL, drop=False, inplace=True)\n",
        "    # We don't want a name on this that will confuse merge operations\n",
        "    df.index.name = None\n",
        "\n",
        "    self.debug(f'{df}')\n",
        "\n",
        "    return df\n",
        "\n",
        "  def create_std_date(self, df, add_columns=True):\n",
        "    \"\"\"\n",
        "    Given a timeseries df with either a DataTimeIndex OR some date indicators,\n",
        "    ensure a standard date column is set.\n",
        "    If a DateTimeIndex exists, use that.\n",
        "    If a date_col is specified, start with that and create a standard datetime\n",
        "    column.\n",
        "    If piecemeal date artifacts are provided as 'year','month','day', use those in\n",
        "    order of preference as given.\n",
        "    Uses pd.to_timestamp().\n",
        "    Does NOT care to retain the DateTimeIndex.\n",
        "\n",
        "    \"\"\"\n",
        "    self.debug('Creating std date columns', banner=True)\n",
        "    #self.debug(f'Given cols: {df.columns}')\n",
        "\n",
        "    # Use cases:\n",
        "    # 1. Have raw data, need to pull together a standard date column\n",
        "    # 2. Have a date column, need to ensure the components exist\n",
        "\n",
        "    if (self.DATE_COL not in list(df.columns)):\n",
        "      # Case 1\n",
        "      # We at least need a 'year' column\n",
        "      if ('year' not in df.columns):\n",
        "        print(df.info())\n",
        "        print(df.head())\n",
        "        raise AssertionError('create_std_date requires a year column')\n",
        "\n",
        "      datestr = []\n",
        "\n",
        "      if ('month' in df.columns):\n",
        "        # Recode as a formatted string\n",
        "        df['month'] = df['month'].astype(dtype='int32')\n",
        "        df['monthStr'] = df['month'].apply(lambda x: f'{x:02}')\n",
        "        datestr.append('{monthStr}/')\n",
        "      else:\n",
        "        datestr.append('01/')\n",
        "        if (add_columns):\n",
        "          df['month'] = 1\n",
        "\n",
        "      if ('day' in df.columns):\n",
        "        # Recode as a formatted string\n",
        "        df['day'] = df['day'].astype(dtype='int32')\n",
        "        df['dayStr'] = df['day'].apply(lambda x: f'{x:02}')\n",
        "        datestr.append('{dayStr}/')\n",
        "      else:\n",
        "        datestr.append('01/')\n",
        "        if (add_columns):\n",
        "          df['day'] = 1\n",
        "\n",
        "      # Year string - Recode as a formatted string\n",
        "      df['year'] = df['year'].astype(dtype='int32')\n",
        "      df['yearStr'] = df['year'].apply(lambda x: f'{x:4}')\n",
        "      datestr.append('{yearStr}')\n",
        "      # Finalize format string\n",
        "      datestr = \"\".join(datestr)\n",
        "      df.info()\n",
        "      self.debug(f'Using date format: {datestr}')\n",
        "      df.info()\n",
        "\n",
        "      # Populate date column\n",
        "      df[self.DATE_COL] = df.apply(lambda x: pd.to_datetime(datestr.format_map(x)), axis=1)\n",
        "    else:\n",
        "      # Case 2\n",
        "      self.debug(f'## Case 2 for df::\\n{df.info}')\n",
        "      # ensure year/month/day cols exist\n",
        "      if ('day' not in df.columns):\n",
        "        df['day'] = df[self.DATE_COL].apply(lambda x: x.day)\n",
        "      if ('year' not in list(df.columns)):\n",
        "        df['year'] = df[self.DATE_COL].apply(lambda x: x.year)\n",
        "      if ('month' not in df.columns):\n",
        "        df['month'] = df[self.DATE_COL].apply(lambda x: x.month)\n",
        "\n",
        "    return df\n",
        "\n",
        "  def snip_dates(self, df):\n",
        "    \"\"\"\n",
        "    Truncates the given df on the set date range.\n",
        "    Requires a standard date column.\n",
        "    Assumes index is NOT a DateTimeIndex. If so, it will be removed when index gets reset.\n",
        "    \"\"\"\n",
        "    self.debug(f'Cropping dates to [{self.START_DATE}:{self.END_DATE}]', banner=True)\n",
        "    if (self.DATE_COL not in df.columns):\n",
        "      raise AssertionError('snip_dates: df requires date column')\n",
        "\n",
        "    df = df[df[self.DATE_COL] >= self.START_DATE]\n",
        "    df = df[df[self.DATE_COL] <= self.END_DATE]\n",
        "\n",
        "    # Reset index and do not store existing as a  column\n",
        "    #df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    self.debug(f'{df}')\n",
        "\n",
        "    return df\n",
        "\n",
        "  def plot(self, df, COLS, filename):\n",
        "    plt.rcParams[\"figure.figsize\"] = [8,6]\n",
        "    plt.plot(df[self.DATE_COL], df[COLS])\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(filename)\n",
        "    plt.title(f'{filename} over Time')\n",
        "    plt.legend(COLS)\n",
        "    plt.show()\n",
        "\n",
        "  def print_time_intervals(self, df):\n",
        "    # Check time intervals\n",
        "    df['interval'] = df[self.DATE_COL] - df[self.DATE_COL].shift(1)\n",
        "    df[[self.DATE_COL, 'interval']].head()\n",
        "    self.debug(\"------ Interval Counts - should be on the month ------\")\n",
        "    self.debug(f\"{df['interval'].value_counts()}\")\n",
        "    df.drop(columns=['interval'], inplace=True)\n",
        "\n",
        "  def trim_columns(self, df, COLS, retain_dates=True):\n",
        "    \"\"\"\n",
        "    Remove all but the given COLS column names from the given df.\n",
        "    If our std date column exists in df, ensure that is also retained.\n",
        "    Returns trimmed df.\n",
        "    \"\"\"\n",
        "    self.debug(f'Trimming columns to {COLS}...', banner=True)\n",
        "    keep_cols = list(COLS)\n",
        "\n",
        "    if (retain_dates):\n",
        "      keep_cols.append(self.DATE_COL)\n",
        "      keep_cols.append('year')\n",
        "      keep_cols.append('month')\n",
        "      keep_cols.append('day')\n",
        "\n",
        "    return df_retain(df, keep_cols)\n",
        "\n",
        "  def downscale(self, df, src_freq, columns=None):\n",
        "    \"\"\"\n",
        "    Downscale the given dataset into the timesteps for this Merger.\n",
        "    Requires input to already contain a std date column.\n",
        "    \"\"\"\n",
        "    self.log(f'Downscaling to freq {self.FREQ}', banner=True)\n",
        "\n",
        "    # Reference target\n",
        "    df_ref = self.get_reference_df()\n",
        "    # use preprocess b/c we only have an index\n",
        "    df_ref = self.preprocess_dates(df_ref)\n",
        "\n",
        "    if (df_ref.shape[0] > df.shape[0]):\n",
        "      print(f'WARN: downscale found missing data in df vs. df_ref: {df.shape[0]} vs. {df_ref.shape[0]}')\n",
        "\n",
        "    if (self.DATE_COL not in df.columns):\n",
        "      raise AssertionError('date column required')\n",
        "\n",
        "    # the columns to work with\n",
        "    COLS = df.columns if columns is None else columns\n",
        "\n",
        "    target_freq = self.FREQ[-1]\n",
        "\n",
        "    # Downscale\n",
        "    if (target_freq == 'Y'):\n",
        "      DATE_COLS = ['year']\n",
        "    elif (target_freq == 'M'):\n",
        "      # want monthly resolution\n",
        "      DATE_COLS = ['year','month']\n",
        "    #TODO support day resolution\n",
        "\n",
        "    # re-code targetFreq to be numeric\n",
        "    target_freq = self.FREQ_SCALE[self.FREQ[-1]]\n",
        "    self.debug(f'src/target freqs: {src_freq}/{target_freq}')\n",
        "\n",
        "    # Compute net result of df downscaling\n",
        "    self.debug(f'Grouping df on {DATE_COLS}')\n",
        "    df_net = df.groupby(DATE_COLS)[COLS].agg(['mean'])\n",
        "\n",
        "    # Having multiple columns in the COLS array creates columns named as tuples\n",
        "    #   - clean this up so we can work with them as normal\n",
        "    df_net = clean_df(df_net)\n",
        "\n",
        "    # This pulls the date out of the index\n",
        "    #df_net.reset_index(inplace=True).rename({'index':'year'}, inplace=True)\n",
        "\n",
        "    # ENSURE that grouped df retains its year/month cols!!\n",
        "    self.debug(f'Grouped df::\\n{df_net}')\n",
        "    self.debug(f'Reference df::\\n{df_ref}')\n",
        "\n",
        "    # Prepare for merge\n",
        "    #df_net = self.create_std_date(df_net)\n",
        "\n",
        "    #df_ref['year'] = df_ref[self.DATE_COL].apply(lambda x: x.year)\n",
        "    #df_ref['month'] = df_ref[self.DATE_COL].apply(lambda x: x.month)\n",
        "\n",
        "    self.debug(f'Joining on DATE_COLS: {DATE_COLS}')\n",
        "    df_merge = df_ref.merge(df_net, how='outer', on=DATE_COLS, suffixes=[None,'net'])\n",
        "\n",
        "    # Remove unnecessary columns and return merged result\n",
        "    df_merge = clean_df(df_merge, ['net'])\n",
        "\n",
        "    # See if there are differences in row size\n",
        "    diff = diff_df_rows(df_ref, df_merge, debug=True)\n",
        "\n",
        "    if (diff > 0):\n",
        "      self.debug(f'Lengths should match: [{df_net.shape[0]},{df_ref.shape[0]}]')\n",
        "      self.debug(f'ISSUE: sizes do not match::\\n---df_net::---\\n{df_net}\\n---df_merge::---\\n{df_merge}')\n",
        "\n",
        "    #if (target_freq > src_freq):\n",
        "      # we are upscaling; we may need to impute some data\n",
        "    for col in COLS:\n",
        "      if df_merge[col].hasnans:\n",
        "        df_merge[col].fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Remove unnecessary columns and return merged result\n",
        "    return df_merge\n",
        "\n",
        "  def upscale(self, df, columns=None):\n",
        "    \"\"\"\n",
        "    Upscale the given dataset into the timesteps for this Merger.\n",
        "    \"\"\"\n",
        "    self.log(f'Upscaling to freq {self.FREQ}', banner=True)\n",
        "\n",
        "    # Reference dest target\n",
        "    df_ref = self.get_reference_df()\n",
        "    # use preprocess b/c we only have an index\n",
        "    df_ref = self.preprocess_dates(df_ref)\n",
        "\n",
        "    if (df_ref.shape[0] < df.shape[0]):\n",
        "      print(f'WARN: upscale found additional data in df vs. df_ref: {df.shape[0]} vs. {df_ref.shape[0]}\\n  these may just be rows outside the merger timeframe')\n",
        "\n",
        "    if (self.DATE_COL not in df.columns):\n",
        "      raise AssertionError('date column required')\n",
        "\n",
        "    # the columns to work with\n",
        "    COLS = df.columns if columns is None else columns\n",
        "\n",
        "    df = self.create_std_date(df)\n",
        "\n",
        "    # Gain an understanding of what periodicity exists\n",
        "    #   in df, b/c we want to select at that level.\n",
        "    src_freq = self.assess_granularity(df, COLS)\n",
        "    target_freq = self.FREQ[-1]\n",
        "\n",
        "    if (target_freq == 'Y'):\n",
        "      raise AssertionError('Cannot upscale to yearly freq! Try downscaling instead.')\n",
        "\n",
        "    # Upscale by joining at the level of resolution of the source dataset\n",
        "    if (src_freq == 'Y'):\n",
        "      DATE_COLS = ['year']\n",
        "    elif (src_freq == 'M'):\n",
        "      DATE_COLS = ['year','month']\n",
        "    else:\n",
        "      DATE_COLS = ['year','month','day']\n",
        "\n",
        "    # Assign values across by joining on right level of freq\n",
        "    df_merge = df_ref.merge(df, how='outer', on=DATE_COLS, suffixes=[None,'net'])\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df_merge = clean_df(df_merge, ['net'])\n",
        "\n",
        "    self.debug(f'Returning upscaled df::\\n{df_merge.head()}')\n",
        "    return df_merge\n",
        "\n",
        "  def assess_granularity(self, df, COLS):\n",
        "    \"\"\"\n",
        "    Assess the level of granularity of the given dataset.\n",
        "    Return single character for year/month/day: Y|M|D\n",
        "    \"\"\"\n",
        "    self.debug(f'assess_granularity for cols: {COLS}', banner=True)\n",
        "\n",
        "    if ('year' not in df.columns):\n",
        "      raise AssertionError('year column required')\n",
        "    if ('month' not in df.columns):\n",
        "      raise AssertionError('month column required')\n",
        "    if ('day' not in df.columns):\n",
        "      raise AssertionError('day column required')\n",
        "\n",
        "    # Compute net number of unique cominations of the given groupings\n",
        "    #    (current) assumption - if result is <= reference size at y/m/d granularity,\n",
        "    #    then the dataset is probably given in that level of granularity\n",
        "    ymd_size = df.groupby(['year','month','day'])[COLS].nunique().shape[0]\n",
        "    self.debug(f'After grouping y/m/d: {ymd_size}')\n",
        "    ym_size = df.groupby(['year','month'])[COLS].nunique().shape[0]\n",
        "    self.debug(f'After grouping y/m: {ym_size}')\n",
        "    y_size = df.groupby(['year'])[COLS].nunique().shape[0]\n",
        "    self.debug(f'After grouping y: {y_size}')\n",
        "\n",
        "    granularity = 'Y'\n",
        "\n",
        "    if (ymd_size > ym_size):\n",
        "      granularity = 'D'\n",
        "    elif(ym_size > y_size):\n",
        "      granularity = 'M'\n",
        "\n",
        "    # Retain full-time output\n",
        "    print(f'Dataset assessed at timestep freq: {granularity}')\n",
        "    return granularity\n"
      ],
      "metadata": {
        "id": "HjMLJtjiQJZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Unit Testing**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sB7ncLMS6QYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MD_UNIT_TEST = False"
      ],
      "metadata": {
        "id": "x-gqM2pq30N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Xazpo9sqfa"
      },
      "outputs": [],
      "source": [
        "if MD_UNIT_TEST:\n",
        "  debug = True\n",
        "\n",
        "  DRIVE_PATH = \"/content/drive/MyDrive/data606\"\n",
        "\n",
        "  # Set the location of this script in GDrive\n",
        "  SCRIPT_PATH = DRIVE_PATH + \"/src/\"\n",
        "\n",
        "  # Root Path of the data on the cloud drive\n",
        "  DATA_ROOT = DRIVE_PATH + \"/data/\"\n",
        "\n",
        "  dataset = \"atmospheric-co2.csv\"\n",
        "  feature_map={'Carbon Dioxide (ppm)':'co2', 'Seasonally Adjusted CO2 (ppm)':'co2_seas'}\n",
        "  date_map={'Year':'year','Month':'month'}\n",
        "\n",
        "  #dataset = \"eruptions-conditioned.csv\"\n",
        "  #feature_map = {'vei':'volcanic_idx'}\n",
        "  #date_map= {'start_year':'year','start_month':'month'}\n",
        "\n",
        "  #dataset = \"WorldForestCover.csv\"\n",
        "  #feature_map = {'PctCover':'pct_forest_cover'}\n",
        "  #date_map= {'Year':'year'}\n",
        "\n",
        "\n",
        "  # Start including data from this date\n",
        "  start_date =  pd.to_datetime(dt.fromisoformat('1990-01-01'))\n",
        "  # Stop including data after this date\n",
        "  end_date = pd.to_datetime(dt.fromisoformat('2020-12-31'))\n",
        "\n",
        "  # Mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  %cd $SCRIPT_PATH\n",
        "\n",
        "  # Load util class\n",
        "  %run -i \"./ProjectUtil.ipynb\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 1 - reference df**"
      ],
      "metadata": {
        "id": "Fru4JRR3JDCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  if MD_UNIT_TEST:\n",
        "    merger = Dataset_Merger(DATA_ROOT, start_date, end_date, debug=True)\n",
        "    df_ref = merger.get_reference_df()\n",
        "    print(df_ref.index)"
      ],
      "metadata": {
        "id": "_oCVSpvcJGQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2 - Full dataset**"
      ],
      "metadata": {
        "id": "jLCXtGn0JI63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df = merger.load_data(dataset)\n",
        "  #print(df.describe().T)\n",
        "\n",
        "  print(df)\n",
        "\n",
        "  COLS = list(feature_map.values())"
      ],
      "metadata": {
        "id": "YYlxklic5hR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2a - preprocess dates**"
      ],
      "metadata": {
        "id": "R-evoA_2JdQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df.rename(columns=feature_map, inplace=True)\n",
        "  df.rename(columns=date_map, inplace=True)\n",
        "\n",
        "  df = merger.preprocess_dates(df)\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "jr4Siwg250o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2b - trim columns**"
      ],
      "metadata": {
        "id": "Q5foKFN8TOV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df = merger.trim_columns(df, COLS)\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "PEIvw4IGTImR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2c - snip dates**"
      ],
      "metadata": {
        "id": "G-TZ_alfKV_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df = merger.snip_dates(df)\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "Jf6YiyzO752D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "    src_freq = merger.assess_granularity(df, COLS)\n",
        "    src_freq = merger.FREQ_SCALE[src_freq]\n"
      ],
      "metadata": {
        "id": "ZfApeCOVUFf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2d - print datetime intervals**"
      ],
      "metadata": {
        "id": "F93vwQJPKYOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  merger.print_time_intervals(df)"
      ],
      "metadata": {
        "id": "P5JqEP_e9nE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2e - downscale**"
      ],
      "metadata": {
        "id": "1uccyx9PKbON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df_dwn = merger.downscale(df, src_freq, columns=COLS)\n",
        "  print(df_dwn)"
      ],
      "metadata": {
        "id": "Ly1XKQdx9uhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # Compare ref dates to these dates\n",
        "  df_ref.shape[0], df_dwn.shape[0]"
      ],
      "metadata": {
        "id": "wA7TRC2E465I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2f - full/merge**"
      ],
      "metadata": {
        "id": "Dl0pR1T2KlKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "\n",
        "  print('merge_dataset() --------------------------------------------------')\n",
        "\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)\n",
        "\n",
        "  df = merger.get_reference_df()\n",
        "  df['val'] = pd.Series(data=[i*0.85 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "SmXQSFSWJV1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df_merge = merger.merge_dataset(dataset,\n",
        "                                feature_map,\n",
        "                                df_aggr=df,\n",
        "                                date_map=date_map)\n",
        "  print(f'---- Result -----------------------------\\n{df_merge}')"
      ],
      "metadata": {
        "id": "1cgRAeRm3oKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3 - merge daily into monthly**"
      ],
      "metadata": {
        "id": "jUzAmsn9qPvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "\n",
        "  print('Case 3: merge your own dataset --------------------------------------------------')\n",
        "\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)\n",
        "  df = merger.get_reference_df()\n",
        "\n",
        "  df['val'] = pd.Series(data=[(i+.17)*0.85 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "twS3hP0vqPvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  merger2 = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='D', debug=True)\n",
        "  df2 = merger2.get_reference_df()\n",
        "\n",
        "  df2['val2'] = pd.Series(data=[(i+.17)*0.35 for i in range(df2.shape[0])], index=df2.index)\n",
        "  df2[merger2.DATE_COL] = df2.index\n",
        "\n",
        "  print(df2)"
      ],
      "metadata": {
        "id": "uw-sCYxWqdIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df_merge = merger.merge_df(df2,\n",
        "                              {'val2':'v2'},\n",
        "                              df_aggr=df)\n",
        "  print(f'---- Result -----------------------------\\n{df_merge}')"
      ],
      "metadata": {
        "id": "PGl9S5jaXXoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 4 - downscale**"
      ],
      "metadata": {
        "id": "uD2asxlM95cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='D', debug=True)\n",
        "  df = merger.get_reference_df()\n",
        "\n",
        "  df['val2'] = pd.Series(data=[(i+.17)*0.35 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  df = merger.preprocess_dates(df)\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "joMUVxN_97W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  merger2 = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)\n",
        "  df_net = merger2.downscale(df, src_freq, ['val2'])\n",
        "  print(df_net)\n",
        "  print('\\nMean at month 02::\\n',df[(df['year'] == 1990) & (df['month'] == '02')].mean())"
      ],
      "metadata": {
        "id": "A2Lz2dUn99u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 5 - upscale Y->M**"
      ],
      "metadata": {
        "id": "ukcXtPZaD12Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # source merger\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='Y', debug=True)\n",
        "  df = merger.get_reference_df()\n",
        "\n",
        "  df['val2'] = pd.Series(data=[(i+.17)*0.35 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "Jd7juZKcD123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # Dest merger\n",
        "  merger2 = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)\n",
        "  df_net = merger2.upscale(df, ['val2'])\n",
        "  print(df_net)\n",
        "  #print('\\nMean at month 02::\\n',df[(df['year'] == 1990) & (df['month'] == '02')].mean())"
      ],
      "metadata": {
        "id": "jv1gl9DeD124"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 5a - upscale Y->M with incomplete time window**"
      ],
      "metadata": {
        "id": "ST8U-jJpgPKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # using merger to create dataset only\n",
        "  data_start_date =  pd.to_datetime(dt.fromisoformat('1993-01-01'))\n",
        "  # Stop including data after this date\n",
        "  data_end_date = pd.to_datetime(dt.fromisoformat('2000-12-31'))\n",
        "\n",
        "  merger = Dataset_Merger(DATA_ROOT, data_start_date, data_end_date, freq='Y', debug=True)\n",
        "  df = merger.get_reference_df()\n",
        "\n",
        "  df['val2'] = pd.Series(data=[(i+.17)*0.35 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "-YfG0GrJgPKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # Dest merger\n",
        "  merger2 = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)\n",
        "  df_net = merger2.upscale(df, ['val2'])\n",
        "  print(df_net)\n",
        "  #print('\\nMean at month 02::\\n',df[(df['year'] == 1990) & (df['month'] == '02')].mean())"
      ],
      "metadata": {
        "id": "cDZnvg67gPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 5b - upscale Y->D**"
      ],
      "metadata": {
        "id": "Gc5aaK0bG-tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # source merger\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='Y', debug=True)\n",
        "  df = merger.get_reference_df()\n",
        "\n",
        "  df['val2'] = pd.Series(data=[i*0.35 for i in range(df.shape[0])], index=df.index)\n",
        "  df[merger.DATE_COL] = df.index\n",
        "\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "2kHRVx3AG-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  # Dest merger\n",
        "  merger2 = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1D', debug=True)\n",
        "  df_net = merger2.upscale(df, ['val2'])\n",
        "  print(df_net)\n",
        "  #print('\\nMean at month 02::\\n',df[(df['year'] == 1990) & (df['month'] == '02')].mean())"
      ],
      "metadata": {
        "id": "qXqEuLHRG-tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 6 - merge of sparse dataset**"
      ],
      "metadata": {
        "id": "LKIIaj6G0Tqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "\n",
        "  print('Case 6:: merge_dataset() --------------------------------------------------')\n",
        "\n",
        "  merger = Dataset_Merger(DATA_ROOT, start_date, end_date, freq='1M', debug=True)"
      ],
      "metadata": {
        "id": "2Q7N37Op0Tqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  df_merge = merger.merge_dataset(dataset,\n",
        "                                  feature_map,\n",
        "                                  date_map=date_map)\n",
        "  print(f'---- Result -----------------------------\\n{df_merge}')"
      ],
      "metadata": {
        "id": "2Jhg861T0Tqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MD_UNIT_TEST:\n",
        "  print(assess_na(df_merge))"
      ],
      "metadata": {
        "id": "RfQ39Sry3uMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3sar_N4dTJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
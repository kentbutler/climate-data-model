{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data606 - Capstone Project\n",
        "```\n",
        "Group H\n",
        "Malav Patel, Kent Butler\n",
        "Prof. Unal Sokaglu\n",
        "```"
      ],
      "metadata": {
        "id": "om0kO_8Wjrdp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi36j3siBTJw"
      },
      "source": [
        "This project is about performing time-series analysis on climate data analysis data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research"
      ],
      "metadata": {
        "id": "ITMtlPsgRtyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "wPKxbg_slFF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some explanations of earth sciences statistics:\n",
        "https://pjbartlein.github.io/REarthSysSci/ltms-and-anomalies.html\n",
        "\n",
        "NOAA PSL NCEP-NCAR datasets:  https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html\n",
        "\n",
        "NOAA PSL, other recognized data sources directory: https://psl.noaa.gov/data/help/othersources/\n",
        "\n",
        "Global environmental policy timeline, https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline\n",
        "\n",
        "OECD convergence of policy, climate,and economy: https://www.oecd.org/\n",
        "\n",
        "NASA climate time machine: https://climate.nasa.gov/interactives/climate-time-machine"
      ],
      "metadata": {
        "id": "PoW6sjUCRvt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factoids"
      ],
      "metadata": {
        "id": "o8PeuAHRlHPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* All of the plastic waste produced in the world in 2019 alone weighs as much as 35,000 Eiffel Towers â€“ 353 million tons  - [*Organization for Economic Cooperation and Development (OECD)*](https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline)\n",
        "\n"
      ],
      "metadata": {
        "id": "WYSxM-qHlJck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv3TRSF9pQXW"
      },
      "source": [
        "## Application Parameters\n",
        "\n",
        "Note: algorithm tuning is done with declaration of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "import datetime"
      ],
      "metadata": {
        "id": "WWu_MYu0lkNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Xazpo9sqfa"
      },
      "outputs": [],
      "source": [
        "debug = False\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/data606\"\n",
        "\n",
        "# Set the location of this script in GDrive\n",
        "SCRIPT_PATH = DRIVE_PATH + \"/src/\"\n",
        "\n",
        "# Model to use\n",
        "MODEL_NAME = \"LSTMv2\"\n",
        "\n",
        "# Journal file\n",
        "JOURNAL_LOG = SCRIPT_PATH + \"cv-results.csv\"\n",
        "\n",
        "# Number of samples to work with - will be split  into train/test\n",
        "SAMPLE_SIZE = 5000\n",
        "\n",
        "# Device to run on\n",
        "run_on_device =  'cpu' # 'cuda'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Predictions**"
      ],
      "metadata": {
        "id": "wwCzMksgGfi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FHrtmWM1HKt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $SCRIPT_PATH"
      ],
      "metadata": {
        "id": "0966NfEwk60W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "4-fpkdHNb595"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "import warnings"
      ],
      "metadata": {
        "id": "fzbBsSdnMPuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-OkNglUVMPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spx1uZxU5ggm"
      },
      "source": [
        "---\n",
        "\n",
        "**Initial Data Load**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load util class\n",
        "%run -i \"./Dataset_Merger.ipynb\""
      ],
      "metadata": {
        "id": "F49q7X3esOwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelExecutor():\n",
        "\n",
        "  def __init__(self, data_path, start_date, end_date, input_window, label_window, test_ratio, val_ratio, num_epochs, model_name, debug=False):\n",
        "    self.debug = debug\n",
        "    self.data_path = data_path\n",
        "    self.start_date = start_date\n",
        "    self.end_date = end_date\n",
        "    self.INPUT_WINDOW = input_window\n",
        "    self.LABEL_WINDOW = label_window\n",
        "    self.TEST_RATIO = test_ratio\n",
        "    self.val_ratio = val_ratio\n",
        "    self.num_epochs = num_epochs\n",
        "    self.MODEL_NAME = model_name\n",
        "\n",
        "\n",
        "  def load_initial_dataset(self, ds_name, feature_map, date_map=None, date_col=None):\n",
        "    # Declare a merger compatible with our source data and our target dataset we want to merge into\n",
        "    self.merger = Dataset_Merger(data_path=self.data_path,\n",
        "                            start_date=self.start_date, end_date=self.end_date,\n",
        "                            debug=self.debug)\n",
        "\n",
        "    # Start by merging initial dataset\n",
        "    if (date_map is not None):\n",
        "      self.df_merge = self.merger.merge_dataset(ds_name, feature_map, date_map=date_map)\n",
        "    elif (date_col is not None):\n",
        "      self.df_merge = self.merger.merge_dataset(ds_name, feature_map, date_col=date_col)\n",
        "\n",
        "    print(assess_na(self.df_merge))\n",
        "\n",
        "\n",
        "  def load_datasets(self, ds_list):\n",
        "    for dataset in ds_list:\n",
        "      if ('date_map' in dataset):\n",
        "        self.df_merge = self.merger.merge_dataset(dataset['filename'],\n",
        "                                        feature_map=dataset['feature_map'],\n",
        "                                        df_aggr=self.df_merge,\n",
        "                                        date_map=dataset['date_map'])\n",
        "      else:\n",
        "        self.df_merge = self.merger.merge_dataset(dataset['filename'],\n",
        "                                    feature_map=dataset['feature_map'],\n",
        "                                    df_aggr=self.df_merge,\n",
        "                                    date_col=dataset['date_col'])\n",
        "\n",
        "      if (self.debug):\n",
        "        print(df_merge)\n",
        "        print(assess_na(df_merge))\n",
        "\n",
        "\n",
        "  def print_correlations(self):\n",
        "    # Assess correlations between all data columns\n",
        "    df_corr = df_merge.corr()\n",
        "\n",
        "    # Identify the columns which have medium to strong correlation with target\n",
        "    df_corr_cols = df_corr[df_corr[TARGET_LABEL] > 0.5]\n",
        "\n",
        "    # Drop the target from the correlation results in case we want to use this reduced set\n",
        "    #    in place of the full set\n",
        "    df_corr_cols = df_corr_cols.drop(columns=[])\n",
        "\n",
        "    # Extract just the column names\n",
        "    corr_cols = df_corr_cols.index.values\n",
        "\n",
        "    if debug:\n",
        "      print(corr_cols)\n",
        "\n",
        "    # Add labels\n",
        "    labels = np.where(np.abs(df_corr) > 0.75, 'S',\n",
        "                      np.where(np.abs(df_corr) > 0.5, 'M',\n",
        "                              np.where(np.abs(df_corr) > 0.25, 'W', '')))\n",
        "    # Plot the matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,\n",
        "                center=0, annot=labels, fmt = '', linewidths = .5,\n",
        "                cmap='vlag', cbar_kws={'shrink':0.8});\n",
        "    plt.title('Heatmap of correlation among variables', fontsize=20)\n",
        "\n",
        "\n",
        "  def process(self):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    from sklearn import preprocessing\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer,  QuantileTransformer, Normalizer\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    import tensorflow as tf\n",
        "\n",
        "    NUM_FEATURES = len(df_merge.columns)\n",
        "\n",
        "    # Capture stats on number of non-metadata columns - so, exclude some stuff\n",
        "    #NET_NUM_FEATURES = len(set(df_merge.columns) - set(['year','month','day']))\n",
        "\n",
        "    # Keep rows aside for post validation?\n",
        "    TOTAL_ROWS = df_merge.shape[0]\n",
        "    NUM_VALIDATION = math.floor(TOTAL_ROWS * VALIDATION_RATIO)\n",
        "    WORKING_ROWS = TOTAL_ROWS - NUM_VALIDATION\n",
        "\n",
        "    # Split non-validation rows into train/test\n",
        "    NUM_TEST = math.floor(WORKING_ROWS * TEST_RATIO)\n",
        "    NUM_TRAIN = WORKING_ROWS - NUM_TEST\n",
        "\n",
        "    print(f'Num features: {NUM_FEATURES}')\n",
        "    print(f'Total rows: {TOTAL_ROWS}')\n",
        "    print(f'Validation rows: {NUM_VALIDATION}')\n",
        "    print(f'Train rows: {NUM_TRAIN}')\n",
        "    print(f'Test rows: {NUM_TEST}'\n",
        "\n",
        "    ## \"\"\"**Split into Train/Test**\"\"\"\n",
        "\n",
        "    df_train = df_merge.iloc[:NUM_TRAIN, :]\n",
        "    df_test = df_merge.iloc[NUM_TRAIN:NUM_TRAIN+NUM_VALIDATION, :]\n",
        "    df_val = df_merge.iloc[NUM_TRAIN+NUM_VALIDATION:, :]\n",
        "\n",
        "    y_train = df_train[TARGET_LABEL]\n",
        "    y_test = df_test[TARGET_LABEL]\n",
        "    y_val = df_val[TARGET_LABEL]\n",
        "\n",
        "    if debug:\n",
        "      print(f'df_train: {df_train.shape}')\n",
        "      print(f'y_train: {y_train.shape}')\n",
        "      print(f'df_test: {df_test.shape}')\n",
        "      print(f'y_test: {y_test.shape}')\n",
        "      print(f'df_val: {df_val.shape}')\n",
        "      print(f'y_val: {y_val.shape}')\n",
        "\n",
        "\n",
        "    ## \"\"\"**Scale data**\n",
        "\n",
        "    # Doing this **after** the split means that training data doesn't get unfair advantage of looking ahead into the 'future' during test & validation.\n",
        "\n",
        "\n",
        "\n",
        "    # Create small pipeline for numerical features\n",
        "    numeric_pipeline = Pipeline(steps = [('impute', SimpleImputer(strategy='mean')),\n",
        "                                        ('scale', MinMaxScaler())])\n",
        "\n",
        "    # get names of numerical features\n",
        "    con_lst = df_train.select_dtypes(include='number').columns.to_list()\n",
        "\n",
        "    # Transformer for applying Pipelines\n",
        "    column_transformer = ColumnTransformer(transformers = [('number', numeric_pipeline, con_lst)])\n",
        "\n",
        "    # Transform data features\n",
        "    X_train_tx = column_transformer.fit_transform(df_train)\n",
        "    X_test_tx = column_transformer.transform(df_test)\n",
        "    X_val_tx = column_transformer.transform(df_val)\n",
        "    X_train_tx.shape, X_test_tx.shape, X_val_tx.shape\n",
        "\n",
        "    # Transform labels\n",
        "    label_scaler = MinMaxScaler()\n",
        "    y_train_tx = label_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    if debug:\n",
        "      print(f'X_train:scaled: {X_train_tx[0]}')\n",
        "      print(f'y_train:scaled: {y_train_tx[0]}')\n",
        "\n",
        "\n",
        "    ## \"\"\"**Extract X and y**\n",
        "\n",
        "    # Normally we would do this by explicitly extracting data from our df.\n",
        "    # However for a time series, we're going to create many small supervised learning sets, so a set of X and y pairs.\n",
        "    # We should end up with data in a shape ready for batched network input:\n",
        "    # `batches X time_steps X features`\n",
        "\n",
        "    if debug:\n",
        "      print(f'X_train_tx: {X_train_tx.shape}')\n",
        "      print(f'y_train_tx: {y_train_tx.shape}')\n",
        "\n",
        "    ## **Modeling**\n",
        "\n",
        "    # These are the features we are going to be modeling\n",
        "    COLS = list(df_merge.columns)\n",
        "\n",
        "    ## \"\"\"**Slice into Batches**\"\"\"\n",
        "\n",
        "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=X_train_tx,\n",
        "    targets=y_train_tx,\n",
        "    sequence_length=self.input_window)\n",
        "\n",
        "    print(ds)\n",
        "\n",
        "    ## \"\"\"**Prep GPU**\"\"\"\n",
        "\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "    ##\"\"\"**Build model**\"\"\"\n",
        "\n",
        "    # Commented out IPython magic to ensure Python compatibility.\n",
        "    # Load model class\n",
        "    # %run -i \"./Model_LSTMv2.ipynb\"\n",
        "\n",
        "    model = ModelLSTMv2(window_size=self.INPUT_WINDOW, num_epochs=self.num_epochs, debug=True)\n",
        "\n",
        "    ##\"\"\"**Train model**\"\"\"\n",
        "\n",
        "    #model.train(X, y, NUM_FEATURES)\n",
        "    model_history = model.train(dataset=ds, num_features=self.num_features)\n",
        "\n",
        "    # Capture stat\n",
        "    num_epochs = len(model_history.history['loss'])\n",
        "\n",
        "    ##\"\"\"**Test Predictions**\"\"\"\n",
        "\n",
        "    num_predictions = y_test.shape[0] - self.INPUT_WINDOW\n",
        "\n",
        "    preds = []\n",
        "    y_test_vals = []\n",
        "\n",
        "    for p in range(num_predictions):\n",
        "      X_pred = X_test_tx[p:p+self.INPUT_WINDOW,:].reshape(-1, self.INPUT_WINDOW, self.NUM_FEATURES)\n",
        "      y_test_vals.append(y_test[p+self.INPUT_WINDOW])\n",
        "      # Predict\n",
        "      pred = model.predict(X_pred)\n",
        "      # Scale and save\n",
        "      preds.append(label_scaler.inverse_transform(pred))\n",
        "\n",
        "    preds = np.array(preds).reshape(num_predictions)\n",
        "\n",
        "    ##\"\"\"**Analyze results**\"\"\"\n",
        "\n",
        "    df_results = pd.DataFrame({'y_test': y_test_vals,\n",
        "                              'preds': preds},\n",
        "                              index=[i+1 for i in range(num_predictions)])\n",
        "\n",
        "    print(df_results)\n",
        "\n",
        "    df_results.plot()\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(y_test_vals, 'blue', linewidth=5)\n",
        "    plt.plot(preds,'r' , linewidth=4)\n",
        "    plt.legend(('Test','Predicted'))\n",
        "    plt.show()\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    from sklearn.metrics import mean_absolute_percentage_error\n",
        "    import csv\n",
        "\n",
        "    # Calculate MAPE\n",
        "    m = tf.keras.metrics.MeanAbsolutePercentageError()\n",
        "    m.update_state(y_test_vals, preds)\n",
        "\n",
        "    mse = mean_squared_error(y_test_vals, preds)\n",
        "    mae = mean_absolute_error(y_test_vals, preds)\n",
        "    mape = m.result().numpy()/100  # adjust Keras output to match scikit\n",
        "\n",
        "    sk_mape = mean_absolute_percentage_error(y_test_vals, preds)\n",
        "\n",
        "    print(f'MSE: {mse}')\n",
        "    print(f'MAE: {mae}')\n",
        "    print(f'MAPE: {mape}')\n",
        "    print(f'SKMAPE: {sk_mape}')\n",
        "\n",
        "    ## \"\"\"**Journal entry**\"\"\"\n",
        "    with open(JOURNAL_LOG, 'w') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      #writer.writerow(['DateTime','Model','TargetLabel','NumFeatures','WindowSize','TestPct','NumEpochs','MSE','MAE','MAPE','SKMAPE','Columns'])\n",
        "      writer.writerow([dt.today().strftime(\"%Y%m%d-%H%M\"),self.MODEL_NAME,self.TARGET_LABEL,self.NUM_FEATURES,self.INPUT_WINDOW,self.TEST_RATIO,num_epochs,mse,mae,mape,sk_mape,COLS])"
      ],
      "metadata": {
        "id": "eKE70OoByrtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
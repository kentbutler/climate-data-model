{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data606 - Capstone Project\n",
        "```\n",
        "Group H\n",
        "Malav Patel, Kent Butler\n",
        "Prof. Unal Sokaglu\n",
        "```"
      ],
      "metadata": {
        "id": "om0kO_8Wjrdp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi36j3siBTJw"
      },
      "source": [
        "This project is about performing time-series analysis on climate data analysis data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research"
      ],
      "metadata": {
        "id": "ITMtlPsgRtyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "wPKxbg_slFF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some explanations of earth sciences statistics:\n",
        "https://pjbartlein.github.io/REarthSysSci/ltms-and-anomalies.html\n",
        "\n",
        "NOAA PSL NCEP-NCAR datasets:  https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html\n",
        "\n",
        "NOAA PSL, other recognized data sources directory: https://psl.noaa.gov/data/help/othersources/\n",
        "\n",
        "Global environmental policy timeline, https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline\n",
        "\n",
        "OECD convergence of policy, climate,and economy: https://www.oecd.org/\n",
        "\n",
        "NASA climate time machine: https://climate.nasa.gov/interactives/climate-time-machine"
      ],
      "metadata": {
        "id": "PoW6sjUCRvt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factoids"
      ],
      "metadata": {
        "id": "o8PeuAHRlHPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* All of the plastic waste produced in the world in 2019 alone weighs as much as 35,000 Eiffel Towers â€“ 353 million tons  - [*Organization for Economic Cooperation and Development (OECD)*](https://www.boell.de/en/2022/05/28/international-environmental-policy-timeline)\n",
        "\n"
      ],
      "metadata": {
        "id": "WYSxM-qHlJck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv3TRSF9pQXW"
      },
      "source": [
        "## Application Parameters\n",
        "\n",
        "Note: algorithm tuning is done with declaration of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "import datetime"
      ],
      "metadata": {
        "id": "WWu_MYu0lkNC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "--Xazpo9sqfa"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/data606\"\n",
        "\n",
        "# Set the location of this script in GDrive\n",
        "SCRIPT_PATH = DRIVE_PATH + \"/src/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Predictions**"
      ],
      "metadata": {
        "id": "wwCzMksgGfi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-FHrtmWM1HKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "29a7e7e1-525d-45da-e47a-f9bedf7d7dbc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $SCRIPT_PATH"
      ],
      "metadata": {
        "id": "0966NfEwk60W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "4-fpkdHNb595"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as plticker\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "import warnings"
      ],
      "metadata": {
        "id": "fzbBsSdnMPuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-OkNglUVMPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spx1uZxU5ggm"
      },
      "source": [
        "---\n",
        "\n",
        "**Initial Data Load**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load util classes\n",
        "%run -i \"./Dataset_Merger.ipynb\"\n",
        "%run -i \"./WindowGenerator.ipynb\"\n",
        "%run -i \"./ModelFactory.ipynb\""
      ],
      "metadata": {
        "id": "F49q7X3esOwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelExecutor():\n",
        "\n",
        "  def __init__(self, data_path, log_path, journal_log, start_date, end_date, input_window, label_window, shift, test_ratio, val_ratio, num_epochs, target_label, model_name, debug=False):\n",
        "    self.debug = debug\n",
        "    self.DATA_PATH = data_path\n",
        "    self.LOG_PATH = log_path\n",
        "    self.JOURNAL_LOG = journal_log\n",
        "    self.START_DATE = start_date\n",
        "    self.END_DATE = end_date\n",
        "    self.INPUT_WINDOW = input_window\n",
        "    self.LABEL_WINDOW = label_window\n",
        "    self.SHIFT = shift\n",
        "    self.TEST_RATIO = test_ratio\n",
        "    self.VAL_RATIO = val_ratio\n",
        "    self.NUM_EPOCHS = num_epochs\n",
        "    self.TARGET_LABEL = target_label\n",
        "    self.TARGET_LABELS = [target_label] # for future expansion\n",
        "    self.MODEL_NAME = model_name\n",
        "\n",
        "    # Device to run on\n",
        "    self.run_on_device =  'cpu' # 'cuda'\n",
        "\n",
        "  def load_initial_dataset(self, ds_name, feature_map, date_map=None, date_col=None):\n",
        "    # Declare a merger compatible with our source data and our target dataset we want to merge into\n",
        "    self.merger = Dataset_Merger(data_path=self.DATA_PATH,\n",
        "                            start_date=self.START_DATE, end_date=self.END_DATE,\n",
        "                            debug=self.debug)\n",
        "\n",
        "    # Start by merging initial dataset\n",
        "    if (date_map is not None):\n",
        "      self.df_merge = self.merger.merge_dataset(ds_name, feature_map, date_map=date_map, add_cyclic=True)\n",
        "    elif (date_col is not None):\n",
        "      self.df_merge = self.merger.merge_dataset(ds_name, feature_map, date_col=date_col, add_cyclic=True)\n",
        "\n",
        "    print(f'#### Feature map: {type(feature_map)} {feature_map.values()}')\n",
        "    self.STEP_FREQ = self.merger.assess_granularity(self.df_merge, list(feature_map.values()))\n",
        "\n",
        "    print(assess_na(self.df_merge))\n",
        "\n",
        "\n",
        "  def load_datasets(self, ds_list):\n",
        "    for dataset in ds_list:\n",
        "      if ('date_map' in dataset):\n",
        "        self.df_merge = self.merger.merge_dataset(dataset['filename'],\n",
        "                                        feature_map=dataset['feature_map'],\n",
        "                                        df_aggr=self.df_merge,\n",
        "                                        date_map=dataset['date_map'])\n",
        "      else:\n",
        "        self.df_merge = self.merger.merge_dataset(dataset['filename'],\n",
        "                                    feature_map=dataset['feature_map'],\n",
        "                                    df_aggr=self.df_merge,\n",
        "                                    date_col=dataset['date_col'])\n",
        "\n",
        "      if (self.debug):\n",
        "        print(self.df_merge)\n",
        "        print(assess_na(self.df_merge))\n",
        "\n",
        "\n",
        "  def print_correlations(self):\n",
        "    # Assess correlations between all data columns\n",
        "    df_corr = self.df_merge.corr()\n",
        "\n",
        "    # Identify the columns which have medium to strong correlation with target\n",
        "    df_corr_cols = df_corr[df_corr[TARGET_LABEL] > 0.5]\n",
        "\n",
        "    # Drop the target from the correlation results in case we want to use this reduced set\n",
        "    #    in place of the full set\n",
        "    df_corr_cols = df_corr_cols.drop(columns=[])\n",
        "\n",
        "    # Extract just the column names\n",
        "    corr_cols = df_corr_cols.index.values\n",
        "\n",
        "    if self.debug:\n",
        "      print(corr_cols)\n",
        "\n",
        "    # Add labels\n",
        "    labels = np.where(np.abs(df_corr) > 0.75, 'S',\n",
        "                      np.where(np.abs(df_corr) > 0.5, 'M',\n",
        "                              np.where(np.abs(df_corr) > 0.25, 'W', '')))\n",
        "    # Plot the matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,\n",
        "                center=0, annot=labels, fmt = '', linewidths = .5,\n",
        "                cmap='vlag', cbar_kws={'shrink':0.8});\n",
        "    plt.title('Heatmap of correlation among variables', fontsize=20)\n",
        "\n",
        "\n",
        "  def current_time_ms(self):\n",
        "    \"\"\"\n",
        "    Return a numeric based on current millisecond.\n",
        "    \"\"\"\n",
        "    return dt.now().microsecond\n",
        "\n",
        "  def process(self):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    from sklearn import preprocessing\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer,  QuantileTransformer, Normalizer\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # It's time to set date as index and remove from dataset\n",
        "    if (self.merger.DATE_COL in self.df_merge.columns):\n",
        "      self.df_merge.set_index(self.merger.DATE_COL, inplace=True, drop=True)\n",
        "\n",
        "    NUM_FEATURES = len(self.df_merge.columns)\n",
        "\n",
        "    # Keep rows aside for post validation?\n",
        "    TOTAL_ROWS = self.df_merge.shape[0]\n",
        "    NUM_VALIDATION = math.floor(TOTAL_ROWS * self.VAL_RATIO)\n",
        "    WORKING_ROWS = TOTAL_ROWS - NUM_VALIDATION\n",
        "\n",
        "    # Split non-validation rows into train/test\n",
        "    NUM_TEST = math.floor(WORKING_ROWS * self.TEST_RATIO)\n",
        "    NUM_TRAIN = WORKING_ROWS - NUM_TEST\n",
        "\n",
        "    print(f'Num features: {NUM_FEATURES}')\n",
        "    print(f'Total rows: {TOTAL_ROWS}')\n",
        "    print(f'Validation rows: {NUM_VALIDATION}')\n",
        "    print(f'Train rows: {NUM_TRAIN}')\n",
        "    print(f'Test rows: {NUM_TEST}')\n",
        "\n",
        "    ## \"\"\"**Split into Train/Test**\"\"\"\n",
        "\n",
        "    df_train = self.df_merge.iloc[:NUM_TRAIN, :]\n",
        "    df_val = self.df_merge.iloc[NUM_TRAIN:NUM_TRAIN+NUM_VALIDATION, :]\n",
        "    df_test = self.df_merge.iloc[NUM_TRAIN+NUM_VALIDATION:, :]\n",
        "\n",
        "    y_train = df_train[TARGET_LABEL]\n",
        "    y_val = df_val[TARGET_LABEL]\n",
        "    y_test = df_test[TARGET_LABEL]\n",
        "\n",
        "    if self.debug:\n",
        "      print(f'df_train: {df_train.shape}')\n",
        "      print(f'y_train: {y_train.shape}')\n",
        "      print(f'df_test: {df_test.shape}')\n",
        "      print(f'y_test: {y_test.shape}')\n",
        "      print(f'df_val: {df_val.shape}')\n",
        "      print(f'y_val: {y_val.shape}')\n",
        "\n",
        "\n",
        "    ## \"\"\"**Scale data**\n",
        "\n",
        "    # Doing this **after** the split means that training data doesn't get unfair advantage of looking ahead into the 'future' during test & validation.\n",
        "\n",
        "    # Create small pipeline for numerical features\n",
        "    numeric_pipeline = Pipeline(steps = [('impute', SimpleImputer(strategy='mean')),\n",
        "                                        ('scale', MinMaxScaler())])\n",
        "\n",
        "    # get names of numerical features\n",
        "    con_lst = df_train.select_dtypes(include='number').columns.to_list()\n",
        "\n",
        "    # Transformer for applying Pipelines\n",
        "    column_transformer = ColumnTransformer(transformers = [('number', numeric_pipeline, con_lst)])\n",
        "\n",
        "    # Transform data features\n",
        "    X_train_tx = column_transformer.fit_transform(df_train)\n",
        "    X_test_tx = column_transformer.transform(df_test)\n",
        "    X_val_tx = column_transformer.transform(df_val)\n",
        "    #X_train_tx.shape, X_test_tx.shape, X_val_tx.shape\n",
        "\n",
        "    # Transform labels\n",
        "    label_scaler = MinMaxScaler()\n",
        "    y_train_tx = label_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Slice labels - we cannot predict anything inside the first INPUT_WINDOW\n",
        "    # WindowGenerator does this for us now\n",
        "    #y_train_tx = y_train_tx[INPUT_WINDOW:]\n",
        "\n",
        "    if self.debug:\n",
        "      print(f'X_train_tx {X_train_tx.shape}: {X_train_tx[0]}')\n",
        "      print(f'y_train_tx {y_train_tx.shape}: {y_train_tx[0]}')\n",
        "\n",
        "    ## \"\"\"**Extract X and y**\n",
        "\n",
        "    # Normally we would do this by explicitly extracting data from our df.\n",
        "    # However for a time series, we're going to create many small supervised learning sets, so a set of X and y pairs.\n",
        "    # We should end up with data in a shape ready for batched network input:\n",
        "    # `batches X time_steps X features`\n",
        "    NUM_LABELS = y_train_tx.shape[1]\n",
        "\n",
        "    ## **Modeling**\n",
        "\n",
        "    # These are the features we are going to be modeling\n",
        "    COLS = list(self.df_merge.columns)\n",
        "\n",
        "    ## \"\"\"**Slice into Batches**\"\"\"\n",
        "\n",
        "    # Use tensorflow util to batch the timeseries\n",
        "    #   note that targets assume first label starts at 0 (vs. targets[INPUT_WINDOW])\n",
        "    # ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    #     data=X_train_tx,\n",
        "    #     targets=y_train_tx,\n",
        "    #     batch_size=self.INPUT_WINDOW*2,\n",
        "    #     sequence_length=self.INPUT_WINDOW)\n",
        "\n",
        "    windower = TfWindowGenerator(input_width=self.INPUT_WINDOW,\n",
        "                                label_width=self.LABEL_WINDOW,\n",
        "                                shift=SHIFT,\n",
        "                                batch_size=self.INPUT_WINDOW,\n",
        "                                debug=False)\n",
        "    print(windower)\n",
        "\n",
        "    # Build TF Dataset from arrays\n",
        "    ds = windower.get_ds_from_arrays(X_train_tx, y_train_tx)\n",
        "\n",
        "    ## \"\"\"**Prep GPU**\"\"\"\n",
        "\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "    ##\"\"\"**Build model**\"\"\"\n",
        "\n",
        "    #model = ModelLSTMv2(window_size=self.INPUT_WINDOW, num_epochs=self.NUM_EPOCHS, debug=True)\n",
        "\n",
        "    # Use factory for flexible selection\n",
        "    mf = ModelFactory(window_size=self.INPUT_WINDOW,label_window=self.LABEL_WINDOW,num_labels=NUM_LABELS,num_epochs=self.NUM_EPOCHS,debug=True)\n",
        "    print (mf)\n",
        "\n",
        "    print(f'Initializing model: {self.MODEL_NAME}')\n",
        "    model = mf.get(self.MODEL_NAME)\n",
        "\n",
        "    ##\"\"\"**Train model**\"\"\"\n",
        "\n",
        "    #model.train(X, y, NUM_FEATURES)\n",
        "    model_history = model.train(dataset=ds, num_features=NUM_FEATURES)\n",
        "\n",
        "    # Capture stat\n",
        "    num_epochs = len(model_history.history['loss'])\n",
        "\n",
        "    ##\"\"\"**Test Predictions**\"\"\"\n",
        "\n",
        "    num_predictions = y_test.shape[0]-self.INPUT_WINDOW-self.LABEL_WINDOW\n",
        "    print(f'Num Exp. Predictions: {num_predictions} == {y_test.shape[0]} - {self.INPUT_WINDOW}')\n",
        "\n",
        "    preds = []\n",
        "    pred_dates = []\n",
        "    y_test_vals = []\n",
        "\n",
        "    # Defaults to a single Day\n",
        "    STEP_OFFSET = pd.DateOffset()\n",
        "\n",
        "    if (self.STEP_FREQ == 'M'):\n",
        "      STEP_OFFSET = pd.DateOffset(months=1)\n",
        "    else:\n",
        "      STEP_OFFSET = pd.DateOffset(years=1)\n",
        "\n",
        "    for p in range(num_predictions):\n",
        "      # Prepare inputs\n",
        "      print(f'Pred range: x_test_tx[{p}:{p+self.INPUT_WINDOW}]')\n",
        "      X_pred = X_test_tx[p:p+self.INPUT_WINDOW,:].reshape(-1, self.INPUT_WINDOW, NUM_FEATURES)\n",
        "\n",
        "      # Prepare outputs\n",
        "      label_start_index = p+self.INPUT_WINDOW\n",
        "      print(f'Exp output: y_test[{label_start_index}:{label_start_index + self.LABEL_WINDOW}]')\n",
        "      y_test_vals.append(y_test[label_start_index:label_start_index + self.LABEL_WINDOW])\n",
        "\n",
        "      if (LABEL_WINDOW == 1):\n",
        "        print(f'Pred date: {df_test.index[label_start_index]}')\n",
        "      else:\n",
        "        print(f'Pred dates: {df_test.index[label_start_index]} + {self.LABEL_WINDOW-1} steps')\n",
        "\n",
        "      # Predict\n",
        "      batch_preds = model.predict(X_pred)\n",
        "      print(f'## Batch step: {batch_preds.shape}')\n",
        "      if (len(batch_preds.shape) > 2):\n",
        "        #batch_preds = batch_preds[0]\n",
        "        batch_preds = batch_preds.reshape(self.LABEL_WINDOW, -1)\n",
        "\n",
        "      # Re-Scale\n",
        "      pred_vals = label_scaler.inverse_transform(batch_preds)\n",
        "      # Reduce to single array\n",
        "      pred_vals = np.squeeze(pred_vals)\n",
        "\n",
        "      # if (len(pred_vals.shape) > 1):\n",
        "      #   #pred_vals = pred_vals.reshape(-1)\n",
        "      #   pred_vals = pred_vals.ravel()\n",
        "      print(f'## Pred vals: {pred_vals.shape}\\n{pred_vals}')\n",
        "\n",
        "      if (LABEL_WINDOW == 1):\n",
        "        preds.append(pred_vals.ravel())\n",
        "        pred_dates.append(df_test.index[label_start_index])\n",
        "      else:\n",
        "        # Add one row per label output; we need to increment the date manually\n",
        "        pred_start_date = df_test.index[label_start_index]\n",
        "        step_date = pred_start_date\n",
        "        for val in pred_vals.tolist():\n",
        "          # add current result values\n",
        "          print(f'## val:  type: {type(val)}  value: {val}')\n",
        "          preds.append(val)\n",
        "          pred_dates.append(step_date)\n",
        "          print(f'## Pred: {step_date} {val}')\n",
        "          # move to next step\n",
        "          step_date = (step_date + STEP_OFFSET)\n",
        "\n",
        "\n",
        "\n",
        "    # for p in range(num_predictions):\n",
        "    #   X_pred = X_test_tx[p:p+self.INPUT_WINDOW,:].reshape(-1, self.INPUT_WINDOW, NUM_FEATURES)\n",
        "    #   y_test_vals.append(y_test[p+self.INPUT_WINDOW+self.LABEL_WINDOW-1])\n",
        "    #   label_index = p+self.INPUT_WINDOW+self.LABEL_WINDOW-1\n",
        "    #  # Predict\n",
        "    #   pred = model.predict(X_pred)\n",
        "    #   # Scale and save\n",
        "    #   preds.append(label_scaler.inverse_transform(pred))\n",
        "    #   pred_dates.append(df_test.index[label_index].strftime('%Y-%m-%d'))\n",
        "\n",
        "    # preds = np.array(preds).reshape(num_predictions)\n",
        "\n",
        "    df_all_results = pd.DataFrame({'preds': preds,\n",
        "                                  'pred_dates':pred_dates,\n",
        "                                   }, index=range(len(preds)))\n",
        "\n",
        "    if (self.LABEL_WINDOW > 1):\n",
        "      # There is probably overlap of output due to this condition\n",
        "      #   Combine predicted outputs for the same dates\n",
        "      # This will put date into the index\n",
        "      df_results = df_all_results.groupby(['pred_dates']).mean()\n",
        "    else:\n",
        "      df_results = df_all_results\n",
        "\n",
        "    # Move dates out of column and into index, if it exists\n",
        "    if ('pred_dates' in df_results.columns):\n",
        "      df_results.set_index('pred_dates', drop=True, inplace=True)\n",
        "\n",
        "    # Reduce df_test to just the columns and dates necessary\n",
        "    df_y = df_retain(df_test, self.TARGET_LABELS)\n",
        "    df_y = df_y[df_results.index.min():df_results.index.max()]\n",
        "\n",
        "    # And merge y values into preds\n",
        "    df_results = df_y.merge(df_results, how='inner', left_index=True, right_index=True, suffixes=['', '_dft'])\n",
        "    df_results = df_results.rename({self.TARGET_LABEL:'y_test'}, axis=1)\n",
        "\n",
        "    # Finally, we have to reduce to a simple index to make the graphing work nicely\n",
        "    df_results.reset_index(inplace=True, drop=False, names='pred_dates')\n",
        "    # But we need a date label col\n",
        "    date_labels = df_results['pred_dates'].apply(lambda x: x.strftime('%Y-%m-%d')).values\n",
        "    # and now we can drop it\n",
        "    df_results.drop(columns=['pred_dates'], inplace=True)\n",
        "\n",
        "\n",
        "    ##\"\"\"**Analyze results**\"\"\"\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    from sklearn.metrics import mean_absolute_percentage_error\n",
        "    import csv\n",
        "\n",
        "    # Timestamp for result set\n",
        "    serial = self.current_time_ms()\n",
        "\n",
        "    # Save model\n",
        "    model.save_model(self.LOG_PATH, serial)\n",
        "\n",
        "    print(df_results)\n",
        "\n",
        "\n",
        "    # Plot results - Y vs. Pred\n",
        "    TICK_SPACING=6\n",
        "    fig, ax = plt.subplots(figsize=(8,6), layout=\"constrained\")\n",
        "    sns.lineplot(data=df_results, ax=ax)\n",
        "    ax.set_xticks(df_results.index, labels=date_labels, rotation=90)\n",
        "    ax.xaxis.set_major_locator(plticker.MultipleLocator(TICK_SPACING))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Temp in degrees C')\n",
        "    plt.legend(('Test','Predicted'))\n",
        "\n",
        "    # write pred results out\n",
        "    df_results['pred_dates'] = date_labels\n",
        "    df_results.to_csv(self.LOG_PATH + f'model-preds-{serial}.csv', index_label='index')\n",
        "\n",
        "\n",
        "    ## \"\"\"**Error Calculations**\"\"\"\n",
        "\n",
        "    y_test_vals = df_results['y_test'].values\n",
        "    preds = df_results['preds'].values\n",
        "\n",
        "    # Calculate MAPE\n",
        "    m = tf.keras.metrics.MeanAbsolutePercentageError()\n",
        "    try:\n",
        "      m.update_state(y_test_vals, preds)\n",
        "    except ValueError as ve:\n",
        "      print(f'ValueError calculating MAPE: {ve}')\n",
        "\n",
        "    mse = mean_squared_error(y_test_vals, preds)\n",
        "    mae = mean_absolute_error(y_test_vals, preds)\n",
        "    mape = m.result().numpy()/100  # adjust Keras output to match scikit\n",
        "    sk_mape = mean_absolute_percentage_error(y_test_vals, preds)\n",
        "\n",
        "    print(f'MSE: {mse}')\n",
        "    print(f'MAE: {mae}')\n",
        "    print(f'MAPE: {mape}')\n",
        "    print(f'SKMAPE: {sk_mape}')\n",
        "\n",
        "    ## \"\"\"**Journal entry**\"\"\"\n",
        "    with open(self.JOURNAL_LOG, 'a') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      #writer.writerow(['DateTime','Model','TargetLabel','NumFeatures','WindowSize','TestPct','NumEpochs','MSE','MAE','MAPE','SKMAPE','Columns'])\n",
        "      writer.writerow([dt.today().strftime(\"%Y%m%d-%H%M\"),serial,self.MODEL_NAME,self.TARGET_LABEL,NUM_FEATURES,self.INPUT_WINDOW,self.TEST_RATIO,num_epochs,mse,mae,mape,sk_mape,COLS])"
      ],
      "metadata": {
        "id": "eKE70OoByrtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A16NMyrHUKmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MzbV7PHprt9jzcoEHRImpenF9dcjuIlG","timestamp":1695179598808}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","\n","## Transformer Model v1\n","\n","---\n","\n","Basic usage of a Transformer."],"metadata":{"id":"MDnCzDVoM0ZE"}},{"cell_type":"code","source":["debug = True\n","DRIVE_PATH = \"/content/drive/MyDrive/data606\"\n","\n","# Set the location of this script in GDrive\n","SCRIPT_PATH = DRIVE_PATH + \"/src/\""],"metadata":{"id":"N__vpGyUYNNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0exLcKsQaUpu","executionInfo":{"status":"ok","timestamp":1700692253332,"user_tz":300,"elapsed":14590,"user":{"displayName":"Kent Butler","userId":"16905390697657349542"}},"outputId":"ee63bb7c-2bae-4ac2-ac34-284eed44c4ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd $SCRIPT_PATH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VNkg-Q5aPGt","executionInfo":{"status":"ok","timestamp":1700692253540,"user_tz":300,"elapsed":211,"user":{"displayName":"Kent Butler","userId":"16905390697657349542"}},"outputId":"c814b114-9f1d-472a-9711-4a1611bb135f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data606/src\n"]}]},{"cell_type":"code","source":["# Load model classes\n","%run -i \"./Model_Base.ipynb\""],"metadata":{"id":"ljl4oVnUYHge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense,RepeatVector, LSTM, Dropout, Reshape\n","from tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D\n","from tensorflow.keras.layers import GlobalAveragePooling1D,LayerNormalization,MultiHeadAttention\n","from keras.callbacks import Callback, EarlyStopping\n","from tensorflow.keras import Input,Model"],"metadata":{"id":"8nFjo6OMngaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class Model_Transformerv1(Base_Model):\n","  \"\"\"\n","  Constructs a model instance. Input data should be a dataframe containing\n","  only the data to be modeled.\n","  \"\"\"\n","  EPSILON = 1e-9\n","\n","  def get_name(self):\n","    return \"TXERv1\"\n","\n","  def train(self, X_train=None, y_train=None, num_features=None, dataset=None):\n","    \"\"\"\n","    Declare model and train.\n","    Based on https://keras.io/examples/timeseries/timeseries_classification_from_scratch/.\n","    \"\"\"\n","    if (dataset is not None):\n","      print(f'### Fitting TXERv1 model to ds {dataset}')\n","      # get batch size\n","      for X,y in dataset:\n","        batch_size = X.shape[-2]\n","        assert not np.any(np.isnan(X))\n","        assert not np.any(np.isnan(y))\n","    else:\n","      print(f'### Fitting TXERv1 model to X {X_train.shape}, y {y_train.shape}')\n","\n","    head_size=256\n","    num_heads=4  # was 4\n","    ff_dim=4 #batch_size  # was 4\n","    num_transformer_blocks=4\n","    mlp_units=[128]\n","    mlp_dropout=0.2\n","    dropout=0.25\n","\n","    print(f'### num features: {num_features}')\n","    print(f'### batch size: {batch_size}')\n","\n","    early_stop = EarlyStopping(monitor = \"loss\", mode = \"min\", patience = 25)\n","\n","    inputs = Input(shape=(batch_size,num_features))\n","    x = inputs\n","    print(f'### input X: {x}')\n","    # Build txer blocks\n","    for _ in range(num_transformer_blocks):\n","        x = self.transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","    # print(f'### after txfrs: {x.shape}')\n","\n","    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    # print(f'### after GlobalVgPool: {x.shape}')\n","    for dim in mlp_units:\n","        x = Dense(dim, activation=\"relu\")(x)\n","        x = Dropout(mlp_dropout)(x)\n","    # print(f'### after mlp: {x.shape}')\n","    # following used softmax; removed during troubleshooting, and probly ok as is\n","    outputs = Dense(self.NUM_LABELS*self.LABEL_WINDOW, activation=\"relu\")(x)\n","    # print(f'### after dense: {outputs.shape}')\n","\n","    if (self.LABEL_WINDOW > 1):\n","     # reshape as => [batch, out_steps, labels]\n","     # print(f'### Reshaping output: {outputs.shape}')\n","     outputs = Reshape([self.LABEL_WINDOW, self.NUM_LABELS])(outputs)\n","     print(f'### Reshaped output: {outputs.shape}')\n","\n","    model = Model(inputs, outputs)\n","    # print(f'## Compiling model')\n","    model.compile(\n","        #loss=\"sparse_categorical_crossentropy\",\n","        loss= tf.keras.losses.Huber(),\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=self.ALPHA),  # orig 1e-4\n","        metrics=[\"sparse_categorical_accuracy\"],\n","        run_eagerly=False,  # for debugging\n","    )\n","    model.summary()\n","\n","    if (dataset is not None):\n","      self.model_hist = model.fit(dataset, epochs=self.NUM_EPOCHS, callbacks = [early_stop], verbose=1)\n","    else:\n","      self.model_hist = model.fit(X_train, y_train, epochs=self.NUM_EPOCHS, verbose=1, callbacks = [early_stop] )\n","\n","    self.model = model\n","    return self.model_hist\n","\n","  def transformer_encoder(self, inputs, head_size, num_heads, ff_dim, dropout=0):\n","    \"\"\"\n","    A transformer block. Usually there are about 4 of these in a complete model.\n","    \"\"\"\n","    # Normalization and Attention\n","    # print(f'### transformer inputs: {inputs.shape}')\n","    x = LayerNormalization(epsilon=self.EPSILON)(inputs)\n","    # print(f'### after Norm x: {x.shape}')\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    # print(f'### after MultiHead x: {x.shape}')\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = LayerNormalization(epsilon=self.EPSILON)(res)\n","    # print(f'### after Norm 2: {x.shape}')\n","    # x = Conv1D(filters=inputs.shape[-1], kernel_size=1, activation=\"relu\")(x)\n","    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    # print(f'### after conv: {x.shape}')\n","    x = Dropout(dropout)(x)\n","    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    # print(f'### after conv 2: {x.shape}')\n","    return x + res"],"metadata":{"id":"RpZVyvzYjJbs"},"execution_count":null,"outputs":[]}]}